{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   136  100   136    0     0   1455      0 --:--:-- --:--:-- --:--:--  1462\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11645  100 11645    0     0   147k      0 --:--:-- --:--:-- --:--:--  147k\n"
     ]
    }
   ],
   "source": [
    "#get the datasets\n",
    "!curl -o test.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/test_small.txt\n",
    "!curl -o train.dat https://raw.githubusercontent.com/huangyanann/CSCE5218/main/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  A11  A12  A13  Unnamed: 13\n",
      "0   1   1   0   0   0   0   0   0   1    1    0    0    1            0\n",
      "1   0   0   1   1   0   1   1   0   0    0    0    0    1            0\n",
      "2   0   1   0   1   1   0   1   0   1    1    1    0    1            1\n",
      "3   0   0   1   0   0   1   0   1   0    1    1    1    1            0\n",
      "4   0   1   0   0   0   0   0   1   1    1    1    1    1            0\n",
      "\n",
      "Test Dataset:\n",
      "   X1  X2  X3\n",
      "1   1   1   1\n",
      "0   0   1   1\n",
      "0   1   1   0\n",
      "0   1   1   0\n",
      "0   1   1   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display the first few rows\n",
    "train_df = pd.read_csv(\"train.dat\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"test.dat\", sep=\"\\t\")\n",
    "\n",
    "print(\"Train Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = []\n",
    "        # Discard header line\n",
    "        f.readline()\n",
    "        for instance in f.readlines():\n",
    "            if not re.search('\\t', instance): continue  # Skip lines that don't have tab-separated values\n",
    "            instance = list(map(int, instance.strip().split('\\t')))\n",
    "            # Add a dummy input so that w0 becomes the bias\n",
    "            instance = [-1] + instance  # -1 for the bias term\n",
    "            data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(weights, instance):\n",
    "    \"\"\"Compute the dot product of weights and instance (ignores the last column which is the label).\"\"\"\n",
    "    return sum(w * x for w, x in zip(weights, instance))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def output(weights, instance):\n",
    "    \"\"\"The output of the model, applying sigmoid to the dot product of the weights and the instance.\"\"\"\n",
    "    in_value = dot_product(weights, instance)\n",
    "    return sigmoid(in_value)\n",
    "\n",
    "\n",
    "def predict(weights, instance):\n",
    "    \"\"\"Predict the label of an instance. If the output is >= 0.5, return 1; otherwise, return 0.\"\"\"\n",
    "    return 1 if output(weights, instance) >= 0.5 else 0\n",
    "\n",
    "\n",
    "def get_accuracy(weights, instances):\n",
    "    \"\"\"Calculate the accuracy of the model by comparing predictions with the true labels.\"\"\"\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "    \"\"\"Train a perceptron using the given instances, learning rate (lr), and number of epochs.\"\"\"\n",
    "    weights = [0] * (len(instances[0]) - 1)  # Initialize weights (one less than the number of columns)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            in_value = dot_product(weights, instance)  # Compute the dot product\n",
    "            out_value = sigmoid(in_value)  # Apply sigmoid to get output\n",
    "            error = instance[-1] - out_value  # Compute error (label - predicted value)\n",
    "\n",
    "            # Update weights using the gradient descent rule\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] += lr * error * out_value * (1 - out_value) * instance[i]  # Weight update rule\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 14 instances): 71.4%\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "\n",
    "# Train the perceptron\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "\n",
    "# Output the results\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the following code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1 \n",
    "In the above code that we have used for train _perceptron(instances, lr, epochs) is to update the weights based on gradient descent using the sigmoid function.\n",
    "1. the sigmoid function produces a continuous value between 0 and 1 , which allows for a smother weight update\n",
    "2. in the snippet predict(weights, instance either gives 0 or 1\n",
    "the code in the snippet would break the gradient descent so we are not using that code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### code\n",
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n",
    "\n",
    "### output\n",
    "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
    "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
    "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 14 instances): 71.4\n",
    "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 42.9\n",
    "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 28.6\n",
    "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 14 instances): 71.4\n",
    "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
    "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 42.9\n",
    "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 28.6\n",
    "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 85.7\n",
    "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 14 instances): 71.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": [
    "### answers\n",
    "# A \n",
    "no, training with full dataset is not always necessary.In some cases , a subset of the dataset can be sufficient for effective learning a better combination of training percentage and hyperparameter yeild gives even better accuracy than the complete dataset.\n",
    "# B\n",
    "With more data, the model takes longer to converge because it needs more updates to minimize the loss. A lower learning rate means each update is smaller, potentially preventing the model from reaching a good minimum within 20 epochs.\n",
    "# C\n",
    "Yes, It is possible to achive higher accuracy by tuning additional hyperparameters such as batch normalization or learning rates implementing advanced techniques like regularization or switching activation function can also help yeild higher accuracy.\n",
    "# D\n",
    "No, training for more epochs is not always beneficial and can sometimes hurt model performance due to overfitting, increased training time, and diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
